{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from utils import random_cv_split, int_col_to_string, string_col_to_int, load_gensim_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "\n",
    "Read the data into a panda dataframe\n",
    "\n",
    "Build training data using the not null data and a prediction set using the null data.\n",
    "\n",
    "Then split up the training data into a training set and a cross validation set by randomly splitting items up by their program priority code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('data/LA_Budget_Data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_data = df[df[\"Expense_Type\"].notnull()]\n",
    "full_data = full_data.dropna(how='any')\n",
    "empty_data = df[False == df[\"Expense_Type\"].notnull()]\n",
    "\n",
    "\n",
    "single_empty = empty_data[empty_data[\"Program_Name\"].notnull()]\n",
    "all_empty = empty_data[False == empty_data[\"Program_Name\"].notnull()]\n",
    "\n",
    "priorities = full_data[\"Program_Priority\"].unique()\n",
    "programs = full_data[\"Program_Name\"].unique()\n",
    "expenses = full_data[\"Expense_Type\"].unique()\n",
    "departments = full_data[\"Dept_Code\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Strings to Ints\n",
    "\n",
    "Before trying to run word2vec, try just running random forest on data giving each string a unique int id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_data = string_col_to_int(\"Program_Priority\", priorities, full_data)\n",
    "full_data = string_col_to_int(\"Expense_Type\", expenses, full_data)\n",
    "full_data = string_col_to_int(\"Program_Name\", programs, full_data)\n",
    "full_data = string_col_to_int(\"Dept_Code\", departments, full_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other than Dept_Code and Program name, there don't appear to be any great correlations with Expense type. I'm not too optimistic about this approach, but let's see how it goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dept_Code</th>\n",
       "      <th>Program_Name</th>\n",
       "      <th>Program_Priority</th>\n",
       "      <th>Appropriation</th>\n",
       "      <th>Fiscal_Year</th>\n",
       "      <th>Expense_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Dept_Code</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.421186</td>\n",
       "      <td>0.028695</td>\n",
       "      <td>0.039360</td>\n",
       "      <td>0.125308</td>\n",
       "      <td>0.270991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Program_Name</th>\n",
       "      <td>0.421186</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200173</td>\n",
       "      <td>0.033100</td>\n",
       "      <td>0.073229</td>\n",
       "      <td>0.050132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Program_Priority</th>\n",
       "      <td>0.028695</td>\n",
       "      <td>0.200173</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.104004</td>\n",
       "      <td>0.062546</td>\n",
       "      <td>0.013889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Appropriation</th>\n",
       "      <td>0.039360</td>\n",
       "      <td>0.033100</td>\n",
       "      <td>0.104004</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.017380</td>\n",
       "      <td>0.091814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fiscal_Year</th>\n",
       "      <td>0.125308</td>\n",
       "      <td>0.073229</td>\n",
       "      <td>0.062546</td>\n",
       "      <td>0.017380</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.127806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Expense_Type</th>\n",
       "      <td>0.270991</td>\n",
       "      <td>0.050132</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.091814</td>\n",
       "      <td>0.127806</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Dept_Code  Program_Name  Program_Priority  Appropriation  \\\n",
       "Dept_Code          1.000000      0.421186          0.028695       0.039360   \n",
       "Program_Name       0.421186      1.000000          0.200173       0.033100   \n",
       "Program_Priority   0.028695      0.200173          1.000000       0.104004   \n",
       "Appropriation      0.039360      0.033100          0.104004       1.000000   \n",
       "Fiscal_Year        0.125308      0.073229          0.062546       0.017380   \n",
       "Expense_Type       0.270991      0.050132          0.013889       0.091814   \n",
       "\n",
       "                  Fiscal_Year  Expense_Type  \n",
       "Dept_Code            0.125308      0.270991  \n",
       "Program_Name         0.073229      0.050132  \n",
       "Program_Priority     0.062546      0.013889  \n",
       "Appropriation        0.017380      0.091814  \n",
       "Fiscal_Year          1.000000      0.127806  \n",
       "Expense_Type         0.127806      1.000000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys:  ['Dept_Code']\n",
      " Acc:  0.573002109210218\n",
      "Keys:  ['Appropriation']\n",
      " Acc:  0.6322314049586777\n",
      "Keys:  ['Program_Name']\n",
      " Acc:  0.5889246323529411\n",
      "Keys:  ['Program_Priority']\n",
      " Acc:  0.49446749654218536\n",
      "Keys:  ['Dept_Code', 'Appropriation']\n",
      " Acc:  0.7438646652370741\n",
      "Keys:  ['Program_Name', 'Program_Priority']\n",
      " Acc:  0.5792183477650362\n"
     ]
    }
   ],
   "source": [
    "guaranteed = [\"Dept_Code\", \"Appropriation\"]\n",
    "not_guaranteed = [\"Program_Name\", \"Program_Priority\"]\n",
    "all_keys = guaranteed + not_guaranteed\n",
    "\n",
    "\n",
    "def train_and_score_rfc(data, train_keys,  iterations=1):\n",
    "    \n",
    "    train, cv = random_cv_split(full_data)\n",
    "    \n",
    "    rfc_low = RandomForestClassifier(n_estimators=10)\n",
    "\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        rfc_low.fit(train[train_keys], train[\"Expense_Type\"])\n",
    "        score = rfc_low.score(cv[train_keys], cv[\"Expense_Type\"])\n",
    "        print(\"Keys: \", train_keys)\n",
    "        print(\" Acc: \", score )\n",
    "\n",
    "    \n",
    "    return rfc_low\n",
    "    \n",
    "rfc = train_and_score_rfc(full_data, [\"Dept_Code\"])\n",
    "rfc = train_and_score_rfc(full_data, [\"Appropriation\"])\n",
    "rfc = train_and_score_rfc(full_data, [\"Program_Name\"])\n",
    "rfc = train_and_score_rfc(full_data, [\"Program_Priority\"])\n",
    "rfc = train_and_score_rfc(full_data, guaranteed)\n",
    "rfc = train_and_score_rfc(full_data, not_guaranteed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So interesting results here: It seems that the best individual indicator is simply Appropriation which makes sense. Then is program name and not far behind Dept code. Last is program priority which makes sense because that was a complex description. So this is kind of good news because for about 200 rows, the only info we have is the dept code and appropriation, so for those rows which we can't run embeddings on, we can still get descent results, given the ~75% accuracy of the only guaranteed keys\n",
    "\n",
    "Now I'm going to try a few combinations just to get a better feel for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys:  ['Program_Name', 'Appropriation']\n",
      " Acc:  0.757351183361224\n",
      "Keys:  ['Program_Priority', 'Appropriation']\n",
      " Acc:  0.6038794110773545\n",
      "Keys:  ['Program_Name', 'Appropriation', 'Dept_Code']\n",
      " Acc:  0.7877647058823529\n",
      "Keys:  ['Dept_Code', 'Appropriation', 'Program_Name', 'Program_Priority']\n",
      " Acc:  0.7599906520215004\n"
     ]
    }
   ],
   "source": [
    "rfc = train_and_score_rfc(full_data, [\"Program_Name\", \"Appropriation\"])\n",
    "rfc = train_and_score_rfc(full_data, [\"Program_Priority\", \"Appropriation\"])\n",
    "rfc = train_and_score_rfc(full_data, [\"Program_Name\", \"Appropriation\", \"Dept_Code\"] )\n",
    "rfc = train_and_score_rfc(full_data, all_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the best combination is essentially all the keys minus program priority which is something to keep in mind for later. I'm interested to see how the random forest results differ with embedding as opposed to unique values.\n",
    "\n",
    "Random Forest was my first instinct to try on this data but I'm gonna try an SVM just in case it is closer to being linearly separablee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC guaranteed data Score:  0.6964368347987043\n",
      "SVC not guaranteed data Score:  0.6881073577047663\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "\n",
    "def train_and_score_svc(data, iterations=1):\n",
    "    \n",
    "    train, cv = random_cv_split(full_data)\n",
    "    \n",
    "    svc_g = svm.SVC()\n",
    "    svc_ng = svm.SVC()\n",
    "    \n",
    "    for i in range(iterations):\n",
    "\n",
    "        svc_g.fit(train[guaranteed], train[\"Expense_Type\"])\n",
    "        score = svc_g.score(cv[guaranteed], cv[\"Expense_Type\"])\n",
    "        print(\"SVC guaranteed data Score: \", score)\n",
    "\n",
    "\n",
    "        svc_ng.fit(train[guaranteed+not_guaranteed], train[\"Expense_Type\"])\n",
    "        score = svc_ng.score(cv[guaranteed+not_guaranteed], cv[\"Expense_Type\"])\n",
    "        print(\"SVC not guaranteed data Score: \", score)\n",
    "    \n",
    "    \n",
    "    return svc_g, svc_ng\n",
    "\n",
    "svc_g, svc_ng = train_and_score_svc(full_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not as good results as Random Forest, but it might scale better. With a smaller dataset I might use svm, but becasue we have around 80% of the data already filled and 20% not filled, I'm leaning towards the random forest\n",
    "\n",
    "Also, SVM performs worse given the program name and ID, because it probably just adds unneccessary complexity towards the fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP\n",
    "\n",
    "A major idea in NLP is the idea that word can be encoded into a vector space. Something interesting about this vector space, however, is that the difference in values (i.e. distance) between similar words and phrases will be smaller than the difference in values between dissimilar words. I'm going to try to leverage these encodings to see if there is a greater theme in either program priority or program names and the expense types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "import logging, urllib.request, zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and load training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Gensim\n",
    "\n",
    "We have to first train Gensim on data in order to load the word2vec of our data. These are some functions to read the zipfile, extract the zipfile, train gensim, and finally save the model under 'mymodel' so we don't have to retrain every time\n",
    "\n",
    "I did packaged all this into a load_gensim_model function which takes in a root path to search for the data and model. If it finds the model it loads it, otherwise, it reads the data, trains the model, saves the model and returns the model. You can read the code in my utils.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "model = load_gensim_model(os.getcwd()+\"/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dim = 300\n",
    "\n",
    "def sentence2vec(words):\n",
    "    \"\"\"\n",
    "    Module that converts takes in a string \n",
    "    with multiple words, takes the word2vec\n",
    "    of each word, and then averages them all\n",
    "    to get a sentence to vec\n",
    "    \n",
    "    For this dataset, no tf-idf is needed\n",
    "    because no word appears twice in a single sentance\n",
    "    \"\"\"\n",
    "    \n",
    "    avg_vec = np.array([\n",
    "                    np.mean([model.wv[w] for w in words.split() if w in model.wv]\n",
    "                            or [np.zeros(dim)], axis=0)\n",
    "                ]).reshape([300])\n",
    "    return avg_vec\n",
    "\n",
    "\n",
    "def add_vector_column(column_name, keys, df):\n",
    "    \n",
    "    vector_keys = []\n",
    "    \n",
    "    for key in keys:\n",
    "        \n",
    "        #print(key)\n",
    "        vector_keys.append(sentence2vec(key))\n",
    "        \n",
    "    #print(vector_keys)\n",
    "    \n",
    "    \n",
    "    col = np.zeros([df.shape[0]] )\n",
    "    \n",
    "    for i, key in enumerate(keys):\n",
    "        index = df[column_name] == key\n",
    "        \n",
    "        np.place(col, index, vector_keys[i])\n",
    "        \n",
    "    \n",
    "    df[column_name+\"_Vectors\"] = col\n",
    "    \n",
    "    \n",
    "        \n",
    "    return df\n",
    "\n",
    "df = add_vector_column(\"Program_Name\", programs, full_data)\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
